{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import argparse\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "from cleaned_est_dataset import make_dataset as est_make_dataset\n",
    "from cleaned_est_dataset import make_dataloader as est_make_dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from metrics import compute_pck_pckh, calculate_error\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some codes are adopted from cite dynamic cnn \n",
    "\n",
    "class Dynamic_conv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False, n_basis_kernels=4,\n",
    "                 temperature=31, pool_dim='freq'):\n",
    "        super(Dynamic_conv2d, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.pool_dim = pool_dim\n",
    "\n",
    "        self.n_basis_kernels = n_basis_kernels\n",
    "        self.attention = attention2d(in_planes, self.kernel_size, self.stride, self.padding, n_basis_kernels,\n",
    "                                     temperature, pool_dim)\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(n_basis_kernels, out_planes, in_planes, self.kernel_size, self.kernel_size),\n",
    "                                   requires_grad=True)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(n_basis_kernels, out_planes))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        for i in range(self.n_basis_kernels):\n",
    "            nn.init.kaiming_normal_(self.weight[i])\n",
    "\n",
    "    def forward(self, x): #x size : [bs, in_chan, frames, freqs]   -> new: [bs, in_chan, freqs, frames]\n",
    "        if self.pool_dim in ['freq', 'chan']:\n",
    "            # softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(4)    # size : [bs, n_ker, 1, frames, 1]\n",
    "            softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(3)    # size : [bs, n_ker, 1,1,frames]\n",
    "        elif self.pool_dim == 'time':\n",
    "            softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(4)    # size : [bs, n_ker, 1, freqs, 1]\n",
    "        elif self.pool_dim == 'both':\n",
    "            softmax_attention = self.attention(x).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)    # size : [bs, n_ker, 1, 1, 1]\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        aggregate_weight = self.weight.view(-1, self.in_planes, self.kernel_size, self.kernel_size) # size : [n_ker * out_chan, in_chan]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            aggregate_bias = self.bias.view(-1)\n",
    "            output = F.conv2d(x, weight=aggregate_weight, bias=aggregate_bias, stride=self.stride, padding=self.padding)\n",
    "        else:\n",
    "            output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding)\n",
    "            # output size : [bs, n_ker * out_chan, frames, freqs]\n",
    "\n",
    "        output = output.view(batch_size, self.n_basis_kernels, self.out_planes, output.size(-2), output.size(-1))\n",
    "        # output size : [bs, n_ker, out_chan, frames, freqs]\n",
    "\n",
    "        if self.pool_dim in ['freq', 'chan']:\n",
    "            assert softmax_attention.shape[-1] == output.shape[-1]\n",
    "        elif self.pool_dim == 'time':\n",
    "            assert softmax_attention.shape[-2] == output.shape[-2]\n",
    "\n",
    "        output = torch.sum(output * softmax_attention, dim=1)  # output size : [bs, out_chan, frames, freqs]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class attention2d(nn.Module):\n",
    "    def __init__(self, in_planes, kernel_size, stride, padding, n_basis_kernels, temperature, pool_dim):\n",
    "        super(attention2d, self).__init__()\n",
    "        self.pool_dim = pool_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        hidden_planes = int(in_planes / 4)\n",
    "\n",
    "        if hidden_planes < 4:\n",
    "            hidden_planes = 4\n",
    "\n",
    "        if not pool_dim == 'both':\n",
    "            self.conv1d1 = nn.Conv1d(in_planes, hidden_planes, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "            self.bn = nn.BatchNorm1d(hidden_planes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.conv1d2 = nn.Conv1d(hidden_planes, n_basis_kernels, 1, bias=True)\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv1d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                if isinstance(m, nn.BatchNorm1d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_planes, hidden_planes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.fc2 = nn.Linear(hidden_planes, n_basis_kernels)\n",
    "\n",
    "\n",
    "    def forward(self, x): #CSI size : [bs, chan, freqs, frames] \n",
    "        if self.pool_dim == 'freq':\n",
    "            x = torch.mean(x, dim=2)  #x size : [bs, chan, frames] \n",
    "        elif self.pool_dim == 'time':\n",
    "            x = torch.mean(x, dim=3)  #x size : [bs, chan, freqs]\n",
    "        elif self.pool_dim == 'both':\n",
    "            # x = torch.mean(torch.mean(x, dim=2), dim=1)  #x size : [bs, chan]\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "        elif self.pool_dim == 'chan':\n",
    "            x = torch.mean(x, dim=1)  #x size : [bs, freqs, frames]\n",
    "\n",
    "        if not self.pool_dim == 'both':\n",
    "            x = self.conv1d1(x)               #x size : [bs, hid_chan, frames]\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv1d2(x)               #x size : [bs, n_ker, frames]\n",
    "        else:\n",
    "            x = self.fc1(x)               #x size : [bs, hid_chan]\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)               #x size : [bs, n_ker]\n",
    "\n",
    "        return F.softmax(x / self.temperature, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 4, 136]              36\n",
      "       BatchNorm1d-2               [-1, 4, 136]               8\n",
      "              ReLU-3               [-1, 4, 136]               0\n",
      "            Conv1d-4               [-1, 5, 136]              25\n",
      "       attention2d-5               [-1, 5, 136]               0\n",
      "    Dynamic_conv2d-6           [-1, 8, 136, 32]           1,080\n",
      "       BatchNorm2d-7           [-1, 8, 136, 32]              16\n",
      "              ReLU-8           [-1, 8, 136, 32]               0\n",
      "         MaxPool2d-9            [-1, 8, 45, 10]               0\n",
      "           Conv1d-10                [-1, 4, 45]              96\n",
      "      BatchNorm1d-11                [-1, 4, 45]               8\n",
      "             ReLU-12                [-1, 4, 45]               0\n",
      "           Conv1d-13                [-1, 5, 45]              25\n",
      "      attention2d-14                [-1, 5, 45]               0\n",
      "   Dynamic_conv2d-15            [-1, 8, 45, 10]           2,880\n",
      "      BatchNorm2d-16            [-1, 8, 45, 10]              16\n",
      "             ReLU-17            [-1, 8, 45, 10]               0\n",
      "        MaxPool2d-18             [-1, 8, 15, 3]               0\n",
      "           Linear-19                  [-1, 128]          46,208\n",
      "      BatchNorm1d-20                  [-1, 128]             256\n",
      "           Linear-21                   [-1, 34]           4,386\n",
      "================================================================\n",
      "Total params: 55,040\n",
      "Trainable params: 55,040\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.94\n",
      "Params size (MB): 0.21\n",
      "Estimated Total Size (MB): 1.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Keypoint Estimator \n",
    "class Estimator(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, shape_data,  \n",
    "                dropout = None,\n",
    "                kernel_size = [3, 3, 3,3,3],\n",
    "                n_kernels =[8, 8, 8,8,8],   \n",
    "                num_layers = 2,  \n",
    "                pad = [1, 1, 1,1,1], #fix la 1,k doi \n",
    "                stride = [1, 1, 1,1,1], #fix la 1, k doi                              \n",
    "                maxPooling = [3,3,2,2,2],\n",
    "                n_basis_kernels = 5, \n",
    "                temperature = 31,\n",
    "                d_linear = 128,\n",
    "                pool_dim = 'time'): \n",
    "        super(Estimator, self).__init__()\n",
    "        \n",
    "        self.num_layers  = num_layers # no. dcnn layers \n",
    "        # self.est_n_input_ch = est_n_input_ch  # no. channels\n",
    "        self.shape_data = shape_data\n",
    "        self.dropout = dropout\n",
    "        self.kernel_size = kernel_size # kernel size\n",
    "        self.n_kernels = n_kernels  # no. filters for each conv layer\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        self.maxPooling = maxPooling   # pooling dimensions for each poooling layers \n",
    "        self.n_basis_kernels = n_basis_kernels  # no. kernels\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.pool_dim = pool_dim # dimension for pooling\n",
    "        self.d_linear = d_linear\n",
    "        # self.est_n_filt_last = est_kernel_size[-1]\n",
    "\n",
    "        \n",
    "        self.h_conv = [0]*self.num_layers\n",
    "        self.w_conv = [0]*self.num_layers\n",
    "        self.h_pool = [0]*self.num_layers\n",
    "        self.w_pool = [0]*self.num_layers\n",
    "        in_dim = [0]*self.num_layers\n",
    "        out_dim = [0]*self.num_layers\n",
    "\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv(i, dropout=None):\n",
    "            in_dim[i] = self.shape_data[0] if i == 0 else self.n_kernels[i-1] #kenh dau vao cua conv\n",
    "            out_dim[i] = self.n_kernels[i] #kenh dau ra cua conv \n",
    "\n",
    "            cnn.add_module( \"conv{0}\".format(i),\n",
    "                            Dynamic_conv2d(in_dim[i],\n",
    "                                           out_dim[i],\n",
    "                                           self.kernel_size[i],\n",
    "                                           self.stride[i],\n",
    "                                           self.pad[i], \n",
    "                                           n_basis_kernels = self.n_basis_kernels, \n",
    "                                           temperature=self.temperature, \n",
    "                                           pool_dim=self.pool_dim)\n",
    "                            ) \n",
    "\n",
    "            \n",
    "            cnn.add_module(\"batchNorm{0}\".format(i), \n",
    "                           nn.BatchNorm2d(out_dim[i], momentum = 0.99)\n",
    "                           )\n",
    "                \n",
    "            cnn.add_module(\"activ{0}\".format(i), \n",
    "                           nn.ReLU())\n",
    "            \n",
    "            if dropout is not None: \n",
    "                cnn.add_module(\"dropout{0}\".format(i),\n",
    "                               nn.Dropout(dropout))\n",
    "                 \n",
    "            cnn.add_module(\"pooling{0}\".format(i), \n",
    "                           nn.MaxPool2d(self.maxPooling[i]))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.h_conv[0] = int((self.shape_data[1] - self.kernel_size[0] + 3 ))\n",
    "                self.w_conv[0] = int((self.shape_data[2] - self.kernel_size[0] + 3)) \n",
    "\n",
    "                self.h_pool[0] = int((self.h_conv[0] - self.maxPooling[0])/self.maxPooling[0] + 1)\n",
    "                self.w_pool[0] = int((self.w_conv[0] - self.maxPooling[0])/self.maxPooling[0] + 1)\n",
    "            \n",
    "            else:\n",
    "                self.h_conv[i] = int((self.h_pool[i-1] - self.kernel_size[i] + 3 ))\n",
    "                self.w_conv[i] = int((self.w_pool[i-1] - self.kernel_size[i] + 3 ))\n",
    "                \n",
    "                self.h_pool[i] = int((self.h_conv[i] - self.maxPooling[i])/self.maxPooling[i] + 1)\n",
    "                self.w_pool[i] = int((self.w_conv[i] - self.maxPooling[i])/self.maxPooling[i] + 1)\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers):  \n",
    "            conv(i, dropout= self.dropout)       \n",
    "        self.cnn = cnn\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features= out_dim[self.num_layers-1]*self.h_pool[self.num_layers-1]*self.w_pool[self.num_layers-1], out_features= self.d_linear)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=self.d_linear)\n",
    "        self.fc2 = nn.Linear(in_features=self.d_linear, out_features=34)\n",
    "        #self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):  # x size : [batch, channel, frames, freqs] 32, 3, 10,114   ```` 32,136 \n",
    "        batch = x.shape[0]\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.cnn(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        # x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.reshape(batch, 17, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Estimator(shape_data = (3,136,32)).to(\"cuda\")\n",
    "\n",
    "summary(model, (3, 136, 32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Estimastor_Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, criterion, optimizer,\n",
    "                 scheduler = None, model_save_path=\"checkpoints\"):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        # self.scheduler = scheduler\n",
    "        self.model_save_path = model_save_path\n",
    "        self.metric = dict()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_epoch_idx = 0\n",
    "        self.val_epoch_idx = 0\n",
    "        self.min_val_loss = 9999\n",
    "\n",
    "    def train_epoch(self):\n",
    "        # print(f\"Training Estimator.  Epoch {self.train_epoch_idx}\")\n",
    "        self.metric[self.train_epoch_idx] = dict()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        for iter, (data, gt) in enumerate(self.train_loader):\n",
    "            # print(data.shape)\n",
    "            data = np.transpose(data, (0, 1, 3, 2))\n",
    "            # print('data_shape', data.shape)\n",
    "            data = data.to(self.device)\n",
    "            confidence = gt[:, :, 2:].to(self.device)\n",
    "            # print('shape confidence', confidence.shape)\n",
    "            label = gt[:, :, 0:2].to(self.device)\n",
    "            # print('shape label', label.shape)\n",
    "            predict = self.model(data)\n",
    "            # print('shape predict', predict.shape)\n",
    "\n",
    "            loss = self.criterion(torch.mul(predict, confidence), torch.mul(label, confidence)) / 32\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            # print(f\"Iter {iter + self.train_epoch_idx * len(self.train_loader)}: MSE - {losses[-1]}\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # self.scheduler.step()\n",
    "\n",
    "        self.metric[self.train_epoch_idx][\"train_loss\"] = sum(losses) / len(losses)\n",
    "        wandb.log({\"Estimator Training Loss\": self.metric[self.train_epoch_idx][\"train_loss\"]})   \n",
    "        self.train_epoch_idx += 1\n",
    "\n",
    "    def val_epoch(self):\n",
    "        # print(f\"Validating Estimator. Epoch {self.val_epoch_idx}\")\n",
    "        self.metric[self.train_epoch_idx] = dict()\n",
    "        self.model.eval()\n",
    "\n",
    "        pck_50_iter = []\n",
    "        pck_40_iter = []\n",
    "        pck_30_iter = []\n",
    "        pck_20_iter = []\n",
    "        pck_10_iter = []\n",
    "        pck_5_iter = []\n",
    "\n",
    "        error_iter = []\n",
    "\n",
    "\n",
    "        losses = []\n",
    "        for data, gt in self.val_loader:\n",
    "            data = np.transpose(data, (0, 1, 3, 2)) # [batch size, channels, freq, time]\n",
    "            data = data.to(self.device)\n",
    "            label = gt[:, :, 0:2].to(self.device) #xy_keypoint\n",
    "\n",
    "            confidence = gt[:, :, 2:3].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                predict = self.model(data)\n",
    "            loss = self.criterion(torch.mul(confidence, predict), torch.mul(confidence, label))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            predict = predict.cpu()\n",
    "            label = label.cpu()\n",
    "            error_iter.append(calculate_error(predict,label))\n",
    "\n",
    "            predict = torch.transpose(predict, 1, 2)\n",
    "            label = torch.transpose(label, 1, 2)\n",
    "\n",
    "            pck_50_iter.append(compute_pck_pckh(predict, label, 0.5))\n",
    "            pck_40_iter.append(compute_pck_pckh(predict, label, 0.4))\n",
    "            pck_30_iter.append(compute_pck_pckh(predict, label, 0.3))\n",
    "            pck_20_iter.append(compute_pck_pckh(predict, label, 0.2))\n",
    "            pck_10_iter.append(compute_pck_pckh(predict, label, 0.1))\n",
    "            pck_5_iter.append(compute_pck_pckh(predict, label, 0.05))\n",
    "\n",
    "           \n",
    "\n",
    "            \n",
    "        error = np.mean(error_iter,0)*1000   \n",
    "        self.metric[self.val_epoch_idx][\"val loss\"] = sum(losses) / len(losses)\n",
    "        self.metric[self.val_epoch_idx][\"pck_50\"] = sum(pck_50_iter) / len(pck_50_iter)\n",
    "        self.metric[self.val_epoch_idx][\"pck_40\"] = sum(pck_40_iter) / len(pck_40_iter)\n",
    "        self.metric[self.val_epoch_idx][\"pck_30\"] = sum(pck_30_iter) / len(pck_30_iter)\n",
    "        self.metric[self.val_epoch_idx][\"pck_20\"] = sum(pck_20_iter) / len(pck_20_iter)\n",
    "        self.metric[self.val_epoch_idx][\"pck_10\"] = sum(pck_10_iter) / len(pck_10_iter)\n",
    "        self.metric[self.val_epoch_idx][\"pck_5\"] = sum(pck_5_iter) / len(pck_5_iter)\n",
    "        self.metric[self.val_epoch_idx][\"mpjpe\"] = error[0]\n",
    "        self.metric[self.val_epoch_idx][\"pampjpe\"] = error[1]\n",
    "\n",
    "        loss_avg = sum(losses) / len(losses)\n",
    "        # print(f\"Val loss: {loss_avg}\")\n",
    "        # print(\"pck_50: \", sum(pck_50_iter) / len(pck_50_iter))\n",
    "        # print(\"pck_40: \", sum(pck_40_iter) / len(pck_40_iter))\n",
    "        # print(\"pck_30: \", sum(pck_30_iter) / len(pck_30_iter))\n",
    "        # print(\"pck_20: \", sum(pck_20_iter) / len(pck_20_iter))\n",
    "        # print(\"pck_10: \", sum(pck_10_iter) / len(pck_10_iter))\n",
    "        # print(\"pck_5: \", sum(pck_5_iter) / len(pck_5_iter))\n",
    "\n",
    "        wandb.log({'estimator_val_loss': loss_avg})\n",
    "        wandb.log({'pck_50 Val': sum(pck_50_iter) / len(pck_50_iter)})\n",
    "        wandb.log({'pck_40 Val': sum(pck_40_iter) / len(pck_40_iter)})\n",
    "        wandb.log({'pck_30 Val': sum(pck_30_iter) / len(pck_30_iter)})\n",
    "        wandb.log({'pck_20 Val': sum(pck_20_iter) / len(pck_20_iter)})\n",
    "        wandb.log({'pck_10 Val': sum(pck_10_iter) / len(pck_10_iter)})\n",
    "        wandb.log({'pck_5 Val': sum(pck_5_iter) / len(pck_5_iter)})\n",
    "        wandb.log({'mpjpe Val': error[0]})\n",
    "        wandb.log({'pampjpe Val': error[1]})\n",
    "     \n",
    "\n",
    "        # logs = \"Val loss: \" + str(sum(losses) / len(losses)) + \", \"\n",
    "        # logs += \"pck_50: \" + str(sum(pck_50_iter) / len(pck_50_iter)) + \", \"\n",
    "        # logs += \"pck_40: \" + str(sum(pck_40_iter) / len(pck_40_iter)) + \", \"\n",
    "        # logs += \"pck_30: \" + str(sum(pck_30_iter) / len(pck_30_iter)) + \", \"\n",
    "        # logs += \"pck_20: \" + str(sum(pck_20_iter) / len(pck_20_iter)) + \", \"\n",
    "        # logs += \"pck_10: \" + str(sum(pck_10_iter) / len(pck_10_iter)) + \", \"\n",
    "        # logs += \"pck_5: \" + str(sum(pck_5_iter) / len(pck_5_iter)) + \"\\n\"\n",
    "        \n",
    "        # with open(\"/home/nxhoang/Work/HPE/src/model/logs/combined_model.txt\", \"a\") as f:\n",
    "        \n",
    "        #     f.write(logs)\n",
    "\n",
    "        if self.val_epoch_idx == 0 or \\\n",
    "                self.metric[self.val_epoch_idx][\"val loss\"] < self.min_val_loss:\n",
    "            self.min_val_loss = self.metric[self.val_epoch_idx][\"val loss\"]\n",
    "            self.save_model()\n",
    "            # print(f\"Estimator Model is saved at epoch {self.val_epoch_idx}\")\n",
    "        self.val_epoch_idx += 1\n",
    "\n",
    "    def test_epoch(self):\n",
    "        # print(f\"Estmator Testing\")\n",
    "        self.model.load_state_dict(torch.load(\"/home/nxhoang/Work/HPE-VinUni/src/model/checkpoints/estimator_best.pth\")['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        pck_50_iter = []\n",
    "        pck_40_iter = []\n",
    "        pck_30_iter = []\n",
    "        pck_20_iter = []\n",
    "        pck_10_iter = []\n",
    "        pck_5_iter = []\n",
    "        error_iter = []\n",
    "        losses = []\n",
    "        for data, gt in tqdm(self.test_loader):\n",
    "            data = np.transpose(data, (0, 1, 3, 2))  # [batch size, channels, freq, time]\n",
    "            data = data.to(self.device)\n",
    "            label = gt[:, :, 0:2].to(self.device)\n",
    "            confidence = gt[:, :, 2:].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                predict = self.model(data)\n",
    "            loss = self.criterion(torch.mul(predict, confidence), torch.mul(label, confidence))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            predict = predict.cpu()\n",
    "            label = label.cpu()\n",
    "            error_iter.append(calculate_error(predict,label))\n",
    "            predict = torch.transpose(predict, 1, 2)\n",
    "            label = torch.transpose(label, 1, 2)\n",
    "\n",
    "            pck_50_iter.append(compute_pck_pckh(predict, label, 0.5))\n",
    "            pck_40_iter.append(compute_pck_pckh(predict, label, 0.4))\n",
    "            pck_30_iter.append(compute_pck_pckh(predict, label, 0.3))\n",
    "            pck_20_iter.append(compute_pck_pckh(predict, label, 0.2))\n",
    "            pck_10_iter.append(compute_pck_pckh(predict, label, 0.1))\n",
    "            pck_5_iter.append(compute_pck_pckh(predict, label, 0.05))\n",
    "\n",
    "            \n",
    "\n",
    "        error = np.mean(error_iter,0)*1000\n",
    "        self.metric[\"test\"] = dict()\n",
    "        self.metric[\"test\"][\"loss\"] = sum(losses) / len(losses)\n",
    "        self.metric[\"test\"][\"pck_50\"] = sum(pck_50_iter) / len(pck_50_iter)\n",
    "        self.metric[\"test\"][\"pck_40\"] = sum(pck_40_iter) / len(pck_40_iter)\n",
    "        self.metric[\"test\"][\"pck_30\"] = sum(pck_30_iter) / len(pck_30_iter)\n",
    "        self.metric[\"test\"][\"pck_20\"] = sum(pck_20_iter) / len(pck_20_iter)\n",
    "        self.metric[\"test\"][\"pck_10\"] = sum(pck_10_iter) / len(pck_10_iter)\n",
    "        self.metric[\"test\"][\"pck_5\"] = sum(pck_5_iter) / len(pck_5_iter)\n",
    "        self.metric[\"test\"][\"mpjpe_mean\"] = error[0]\n",
    "        self.metric[\"test\"][\"pampjpe_mean\"] = error[1]\n",
    "        \n",
    "\n",
    "        wandb.log({'Estimator Loss Test': sum(losses) / len(losses)})\n",
    "        wandb.log({'pck_50': sum(pck_50_iter) / len(pck_50_iter)})\n",
    "        wandb.log({'pck_40': sum(pck_40_iter) / len(pck_40_iter)})\n",
    "        wandb.log({'pck_30': sum(pck_30_iter) / len(pck_30_iter)})\n",
    "        wandb.log({'pck_20': sum(pck_20_iter) / len(pck_20_iter)})\n",
    "        wandb.log({'pck_10': sum(pck_10_iter) / len(pck_10_iter)})\n",
    "        wandb.log({'pck_5': sum(pck_5_iter) / len(pck_5_iter)})\n",
    "        wandb.log({'mpjpe': error[0]})\n",
    "        wandb.log({'pampjpe': error[1]})\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        state_dict = dict()\n",
    "        state_dict[\"model_state_dict\"] = self.model.state_dict()\n",
    "        state_dict[\"optimizer_state_dict\"] = self.optimizer.state_dict()\n",
    "        state_dict[\"train_loss_history\"] = torch.tensor([self.metric[i][\"train_loss\"] for i in range(self.train_epoch_idx)])\n",
    "        state_dict[\"val_loss_history\"] = torch.tensor([self.metric[i][\"val loss\"] for i in range(self.val_epoch_idx)])\n",
    "        state_dict[\"pck_50\"] = self.metric[self.val_epoch_idx][\"pck_50\"]\n",
    "        state_dict[\"pck_40\"] = self.metric[self.val_epoch_idx][\"pck_40\"]\n",
    "        state_dict[\"pck_30\"] = self.metric[self.val_epoch_idx][\"pck_30\"]\n",
    "        state_dict[\"pck_20\"] = self.metric[self.val_epoch_idx][\"pck_20\"]\n",
    "        state_dict[\"pck_10\"] = self.metric[self.val_epoch_idx][\"pck_10\"]\n",
    "        state_dict[\"pck_5\"] = self.metric[self.val_epoch_idx][\"pck_5\"]\n",
    "        state_dict[\"mpjpe\"] = self.metric[self.val_epoch_idx][\"mpjpe\"]\n",
    "        state_dict[\"pampjpe\"] = self.metric[self.val_epoch_idx][\"pampjpe\"]\n",
    "        save_path = os.path.join(self.model_save_path, f\"model_best.pth\")\n",
    "        torch.save(state_dict, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nxhoang/Work/HPE-VinUni/src/model/configs/BO_Estimator.yaml') as f:\n",
    "  sweep_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_config = {'modality': 'wifi-csi',\n",
    "                        'protocol': 'protocol1',\n",
    "                        'data_unit': 'frame',\n",
    "                        'random_split': {'ratio': 0.8,\n",
    "                                         'random_seed': 42,\n",
    "                                         'train_dataset': {'split': 'training',\n",
    "                                                            'scenes': 'None',\n",
    "                                                            'subjects': 'None',\n",
    "                                                          'actions': 'all'},\n",
    "                                          'val_dataset': {'split': 'validation',\n",
    "                                                          'scenes': 'None',\n",
    "                                                          'subjects': 'None',\n",
    "                                                          'actions': 'all'}},\n",
    "                        'cross_scene_split': {'train_dataset': {'split': 'training',\n",
    "                                              'scenes': ['E01', 'E02', 'E03'],\n",
    "                                              'subjects': 'None',\n",
    "                                              'actions': 'all'},\n",
    "                                              'val_dataset': {'split': 'validation',\n",
    "                                                              'scenes': ['E04'],\n",
    "                                                              'subjects': 'None',\n",
    "                                                              'actions': 'all'}},\n",
    "                        'cross_subject_split': {'train_dataset': {'split': 'training',\n",
    "                                                                  'scenes': 'None',\n",
    "                                                                  'subjects': ['S01','S02','S03','S04','S06','S07','S08','S09','S11','S12','S13','S14','S16','S17','S18','S19','S21','S22','S23','S24','S26','S27','S28','S29','S31','S32','S33','S34','S36','S37','S38','S39'],\n",
    "                                                                  'actions': 'all'},\n",
    "                                                'val_dataset': {'split': 'validation',\n",
    "                                                                'scenes': 'None',\n",
    "                                                                'subjects': ['S05', 'S10', 'S15', 'S20', 'S25', 'S30', 'S35', 'S40'],\n",
    "                                                                'actions': 'all'}},\n",
    "                        'manual_split': {'train_dataset': {'split': 'training',\n",
    "                                                           'scenes': 'None',\n",
    "                                                           'subjects': ['S01','S02','S03','S04','S05','S06','S07','S08','S09','S10','S11','S12','S13','S14','S15','S16','S17','S18','S19','S20','S21','S22','S23','S24','S25','S26','S27','S28','S29','S30','S31','S32','S33','S34','S35','S36','S37','S38','S39','S40'],\n",
    "                                                           'actions': ['A01','A02','A03','A04','A05','A06','A07','A08','A09','A10','A11','A12','A13','A14','A15','A16','A17','A18','A19','A20','A21']},\n",
    "                                          'val_dataset': {'split': 'validation',\n",
    "                                                          'scenes': 'None',\n",
    "                                                          'subjects': ['S01','S02','S03','S04','S05','S06','S07','S08','S09','S10','S11','S12','S13','S14','S15','S16','S17','S18','S19','S20','S21','S22','S23','S24','S25','S26','S27','S28','S29','S30','S31','S32','S33','S34','S35','S36','S37','S38','S39','S40'],\n",
    "                                                          'actions': ['A22', 'A23', 'A24', 'A25', 'A26', 'A27']}},\n",
    "                        'split_to_use': 'random_split',\n",
    "                        'init_rand_seed': 0,\n",
    "                        'data_root': '/home/nxhoang/Work/HPE-VinUni/Data',\n",
    "                        'AE_checkpoint': '/home/nxhoang/Work/HPE-VinUni/src/model/checkpoints/AE_model_best.pth',\n",
    "                        'combined_checkpoint': '/home/nxhoang/Work/HPE-VinUni/src/model/checkpoints/combined_model_best.pth'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "def training():\n",
    "     with wandb.init(config=sweep_config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # start = time.perf_counter()\n",
    "        \n",
    "        cnn_input_shape = (3,136,32)\n",
    "        # print(\"data_shape\",)\n",
    "        # Define device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        estimator = Estimator(shape_data=cnn_input_shape, \n",
    "                              dropout=config['est_dropout'],\n",
    "                              n_basis_kernels=config['est_n_basis_kernels'],\n",
    "                              kernel_size=(config['est_kernel_size_1'], config['est_kernel_size_2'], config['est_kernel_size_3']),\n",
    "                              n_kernels=(config['est_n_kernels_1'], config['est_n_kernels_2'], config['est_n_kernels_3']),\n",
    "                              num_layers=config['est_num_layers'],\n",
    "                              pad=[1,1,1,1,1],\n",
    "                              stride = [1,1,1,1,1],\n",
    "                              maxPooling= (config['est_maxpooling_1'],  config['est_maxpooling_2'], config['est_maxpooling_3']),\n",
    "                              temperature=config['est_temperature'], \n",
    "                              pool_dim='time',\n",
    "                              d_linear = config['est_d_hidden']\n",
    "                              )\n",
    "\n",
    "        est_train_dataset, est_test_dataset = est_make_dataset(other_config[\"data_root\"], other_config)\n",
    "        rng_generator1 = torch.manual_seed(other_config['init_rand_seed'])\n",
    "        train_loader1 = est_make_dataloader(est_train_dataset, is_training=True, generator=rng_generator1,\n",
    "                                                 batch_size = config['est_batch_size'])\n",
    "        val_data1, test_data1 = train_test_split(est_test_dataset, test_size=0.5, random_state=41)\n",
    "        val_loader1 = est_make_dataloader(val_data1, is_training=False, generator=rng_generator1,\n",
    "                                               batch_size = config['est_batch_size'])\n",
    "        test_loader1 = est_make_dataloader(test_data1, is_training=False, generator=rng_generator1,\n",
    "                                                batch_size = config['est_batch_size'])\n",
    "\n",
    "        # Training combined_model with Encoder parameters being frozen\n",
    "        criterion_est = nn.MSELoss().to(device)\n",
    "        # optimizer_cb = torch.optim.SGD(combined_model.parameters(), lr=config['est_lr'], momentum=config['est_momentum'])\n",
    "        optimizer_est = torch.optim.Adam(estimator.parameters(), lr=config['est_lr'])\n",
    "\n",
    "        n_epochs_est = config[\"est_n_epochs\"]\n",
    "      \n",
    "        # n_epochs_decay = 60\n",
    "        # epoch_count = 1\n",
    "\n",
    "        # def lambda_rule(epoch):\n",
    "        #     lr_l = 1.0 - max(0, epoch + epoch_count - n_epochs_cb) / float(n_epochs_decay + 1)\n",
    "        #     return lr_l\n",
    "\n",
    "        # scheduler_cb = torch.optim.lr_scheduler.LambdaLR(optimizer_cb, lr_lambda=lambda epoch: 1.0 - max(0,\n",
    "        #                                                                                                  epoch + epoch_count - n_epochs_cb) / float(\n",
    "        #     n_epochs_decay + 1))\n",
    "\n",
    "\n",
    "        trainer_est = Estimastor_Trainer(estimator, train_loader1, val_loader1, test_loader1, criterion= criterion_est,\n",
    "                                        optimizer = optimizer_est, scheduler=None)\n",
    "        \n",
    "        print(\"START TRAINING ESTIMATOR AT: \", time.perf_counter_ns())\n",
    "        for epoch in range(n_epochs_est):\n",
    "            trainer_est.train_epoch()\n",
    "            trainer_est.val_epoch()\n",
    "        print(\"END TRAINING ESTIMATOR AT: \", time.perf_counter_ns())    \n",
    "        trainer_est.test_epoch()\n",
    "\n",
    "        config['estimator_loss'] = trainer_est.metric[\"test\"][\"loss\"] \n",
    "        if trainer_est.metric[\"test\"][\"loss\"] < 0.1:\n",
    "            wandb.alert(\n",
    "                title='Xuan Hoang just found the best loss_val',\n",
    "                text=f'Val Loss{trainer_est.metric[\"test\"][\"loss\"]} is below 0.1',\n",
    "            )\n",
    "            print('Alert triggered') \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kg5jin7d\n",
      "Sweep URL: https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep = sweep_config, project =\"Estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thứ ba, 14 Tháng 5 năm 2024 16:05:12 +07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e0kzs96f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_d_hidden: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_dropout: 0.12881594491118578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_1: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_2: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_3: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_lr: 0.004191271302467152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_1: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_2: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_3: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_basis_kernels: 172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_epochs: 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_2: 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_3: 106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_temperature: 33\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nxhoang/Work/HPE-VinUni/src/model/wandb/run-20240514_160514-e0kzs96f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nxhoang/Estimator/runs/e0kzs96f' target=\"_blank\">fresh-sweep-1</a></strong> to <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">https://wandb.ai/nxhoang/Estimator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nxhoang/Estimator/runs/e0kzs96f' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/runs/e0kzs96f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ESTIMATOR AT:  88878407030663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-1a7298fdbb29>\", line 61, in training\n",
      "    trainer_est.train_epoch()\n",
      "  File \"<ipython-input-6-b382f05b3095>\", line 33, in train_epoch\n",
      "    predict = self.model(data)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-5-a9356ea502dc>\", line 104, in forward\n",
      "    x = self.cnn(x)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-4-01e62b4567c2>\", line 35, in forward\n",
      "    softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(4)    # size : [bs, n_ker, 1, freqs, 1]\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-4-01e62b4567c2>\", line 105, in forward\n",
      "    x = self.conv1d1(x)               #x size : [bs, hid_chan, frames]\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 313, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 310, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b10dc5d2101436388f0255cbaeeba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-1</strong> at: <a href='https://wandb.ai/nxhoang/Estimator/runs/e0kzs96f' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/runs/e0kzs96f</a><br/> View project at: <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">https://wandb.ai/nxhoang/Estimator</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240514_160514-e0kzs96f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run e0kzs96f errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"<ipython-input-9-1a7298fdbb29>\", line 61, in training\n",
      "    trainer_est.train_epoch()\n",
      "  File \"<ipython-input-6-b382f05b3095>\", line 33, in train_epoch\n",
      "    predict = self.model(data)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-5-a9356ea502dc>\", line 104, in forward\n",
      "    x = self.cnn(x)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-4-01e62b4567c2>\", line 35, in forward\n",
      "    softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(4)    # size : [bs, n_ker, 1, freqs, 1]\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-4-01e62b4567c2>\", line 105, in forward\n",
      "    x = self.conv1d1(x)               #x size : [bs, hid_chan, frames]\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 313, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 310, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run e0kzs96f errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-9-1a7298fdbb29>\", line 61, in training\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer_est.train_epoch()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-6-b382f05b3095>\", line 33, in train_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     predict = self.model(data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-5-a9356ea502dc>\", line 104, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.cnn(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     input = module(input)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-01e62b4567c2>\", line 35, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     softmax_attention = self.attention(x).unsqueeze(2).unsqueeze(4)    # size : [bs, n_ker, 1, freqs, 1]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-01e62b4567c2>\", line 105, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1d1(x)               #x size : [bs, hid_chan, frames]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 313, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/nxhoang/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 310, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.padding, self.dilation, self.groups)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g93q3caw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_d_hidden: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_dropout: 0.19099679901513117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_1: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_kernel_size_3: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_lr: 0.007167629831044602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_1: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_maxpooling_3: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_basis_kernels: 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_epochs: 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_1: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_2: 47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_n_kernels_3: 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \test_temperature: 31\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nxhoang/Work/HPE-VinUni/src/model/wandb/run-20240514_160547-g93q3caw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nxhoang/Estimator/runs/g93q3caw' target=\"_blank\">warm-sweep-2</a></strong> to <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">https://wandb.ai/nxhoang/Estimator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/sweeps/kg5jin7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nxhoang/Estimator/runs/g93q3caw' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/runs/g93q3caw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-1a7298fdbb29>\", line 27, in training\n",
      "    est_train_dataset, est_test_dataset = est_make_dataset(other_config[\"data_root\"], other_config)\n",
      "  File \"/home/nxhoang/Work/HPE-VinUni/src/model/est_dataset.py\", line 346, in make_dataset\n",
      "    train_dataset = MMFi_Dataset(database, config['data_unit'], **config_dataset['train_dataset'])\n",
      "  File \"/home/nxhoang/Work/HPE-VinUni/src/model/est_dataset.py\", line 138, in __init__\n",
      "    self.data_list = self.load_data()\n",
      "  File \"/home/nxhoang/Work/HPE-VinUni/src/model/est_dataset.py\", line 195, in load_data\n",
      "    if os.path.getsize(data_dict[mod+'_path']) == 0:\n",
      "  File \"/home/nxhoang/anaconda3/lib/python3.7/genericpath.py\", line 50, in getsize\n",
      "    return os.stat(filename).st_size\n",
      "Exception\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8df7a22a6b4e56aeafd7507d24b61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-sweep-2</strong> at: <a href='https://wandb.ai/nxhoang/Estimator/runs/g93q3caw' target=\"_blank\">https://wandb.ai/nxhoang/Estimator/runs/g93q3caw</a><br/> View project at: <a href='https://wandb.ai/nxhoang/Estimator' target=\"_blank\">https://wandb.ai/nxhoang/Estimator</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240514_160547-g93q3caw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!date\n",
    "wandb.agent(sweep_id, function=training, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
